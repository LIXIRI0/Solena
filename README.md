
---

# ğŸŒ™ Solena â€” A Minimal GPT-Style Transformer

Solena is a tiny educational GPT-like language model built completely from scratch using PyTorch.
It trains on plain text, learns next-character prediction, and can generate coherent sequences.

This project is made to be:

ğŸ”¥ Simple â€” minimal, readable architecture

ğŸ§ª Hackable â€” ideal for learning LLM internals

â™»ï¸ Resumable â€” seamless checkpoint save/load

âš¡ Lightweight â€” runs even on weak CPUs / WSL

ğŸ¯ Consistent â€” tokenizer & vocab always align



---

## ğŸ“ Project Structure
```
Solena/
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ solena_tiny.py
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ tokenizer.py
â”‚   â””â”€â”€ dataset.py
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw.txt
â”‚
â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ SolenaTiny.pth
â”‚
â”œâ”€â”€ train.py
â”œâ”€â”€ generate.py
â”œâ”€â”€ config.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

### ğŸš€ Installation

Requires Python 3.9+
```
pip install -r requirements.txt
```
If you're on WSL:
```
sudo apt update
sudo apt install build-essential python3-dev
```

---

### ğŸ“¦ Add Training Data

Place your text dataset at:
```
data/raw.txt
```
You can train on:

Shakespeare

Song lyrics

Books

Chat logs

Your own writing

ANY plain-text file



---

### ğŸ§  Training

Just run:
```
python3 train.py
```
The trainer automatically:

Loads config

Resumes from checkpoint if RESUME=True

Saves only the best model if SAVE_BEST_ONLY=True

Supports fractional dataset training (fast debug)


You can run it multiple times â€” it will continue training seamlessly.


---

### ğŸ“ Text Generation

After training:
```
python3 generate.py
```
Example:
```
prompt> hello
----
helolo hera sor thi...
```
As loss decreases, output quality improves.


---

### âš™ï¸ Configuration (config.py)

All model & training parameters live inside config.py, including:

Sequence length (SEQ_LEN)

Batch size (BATCH_SIZE)

Learning rate (LR)

Embedding dims, layers, heads

CPU/GPU automatic detection

Checkpoint behavior

Train subset fraction (TRAIN_FRACTION)

Dev/debug modes


You can modify anything at any time.


---

### ğŸ”§ Example Dev Mode Settings
```
SEQ_LEN        = 16
BATCH_SIZE     = 16
EMBED_DIM      = 32
N_HEADS        = 1
N_LAYERS       = 1
EPOCHS_PER_RUN = 10
TRAIN_FRACTION = 0.1
LR             = 3e-4
```
Perfect for weak hardware or WSL.


---

ğŸ§ª Example Output
```
prompt> To be or not to be
----
To be or not to beren tomas hir...
```
(Improves significantly over training.)


---

## ğŸ›£ï¸ Roadmap

[ ] Add dropout

[ ] Add learned/rope positional encodings

[ ] Add attention mask

[ ] Add perplexity evaluation

[ ] Add sampling options (top-k, nucleus, temp)

[ ] Add web UI for inference

[ ] Multi-GPU support for cloud GPUs (A10G / T4)



---

###  ğŸ¤ Contributing

PRs, issues, and improvements are welcome.
Solena is intentionally minimal to encourage learning and experimentation.


---

### âš–ï¸ License

MIT License


---

# ğŸ§¡ Solena is just the beginning.
